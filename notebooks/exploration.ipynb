{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/nathalie/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/nathalie/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from Levenshtein import distance\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import words, brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(words.words())\n",
    "word_frequencies = Counter(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Remove punctuation and lowercase text.\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word.lower() for word in brown.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Counter(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My friend Anne is last runner.\n"
     ]
    }
   ],
   "source": [
    "word_frequencies = Counter(brown.words())  # Word frequencies from the Brown corpus\n",
    "\n",
    "model = Counter(ngrams(tokens, 3))\n",
    "\n",
    "input = 'my friend anne is dast runner'\n",
    "input_preprocessed = preprocess_text(input)\n",
    "words = input_preprocessed.split()\n",
    "\n",
    "corrected_words = []\n",
    "prev_word = None\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    suggestions = [\n",
    "        (vocab_word, distance(word, vocab_word), word_frequencies[vocab_word])\n",
    "        for vocab_word in vocabulary\n",
    "    ]\n",
    "    suggestions = sorted(suggestions, key=lambda x: (x[1], -x[2]))\n",
    "    suggestions = [suggestion[0] for suggestion in suggestions[0:5]]\n",
    "\n",
    "    # Initialize variables\n",
    "    best_suggestion = suggestions[0]\n",
    "    best_combined_prob = 0\n",
    "\n",
    "    # Get the next word (if it exists)\n",
    "    next_word = words[i + 1] if i + 1 < len(words) else None\n",
    "\n",
    "    for suggestion in suggestions:\n",
    "        # Calculate bigram probability with the previous word\n",
    "        prob_prev = 0\n",
    "        if prev_word:\n",
    "            bigram_count = model.get((prev_word, suggestion), 0)\n",
    "            unigram_count = sum(1 for bigram in model if bigram[0] == prev_word)\n",
    "            prob_prev = bigram_count / unigram_count if unigram_count > 0 else 0\n",
    "\n",
    "        # Calculate bigram probability with the next word\n",
    "        prob_next = 0\n",
    "        if next_word:\n",
    "            bigram_count = model.get((suggestion, next_word), 0)\n",
    "            unigram_count = sum(1 for bigram in model if bigram[0] == suggestion)\n",
    "            prob_next = bigram_count / unigram_count if unigram_count > 0 else 0\n",
    "\n",
    "        # Combine probabilities (you can adjust weights as needed)\n",
    "        combined_prob = (prob_prev + prob_next) / 2\n",
    "\n",
    "        # Update best suggestion based on combined probability\n",
    "        if combined_prob > best_combined_prob:\n",
    "            best_combined_prob = combined_prob\n",
    "            best_suggestion = suggestion\n",
    "\n",
    "    corrected_words.append(best_suggestion)\n",
    "    prev_word = best_suggestion  # Update previous word\n",
    "\n",
    "corrected_text = ' '.join(corrected_words).capitalize() + '.'\n",
    "\n",
    "doc = nlp(corrected_text)\n",
    "for ent in doc.ents:\n",
    "    corrected_text = corrected_text.replace(ent.text, ent.text.capitalize())\n",
    "\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kyle 10 14 PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"My friend anne is a fast runner\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spellchecker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
