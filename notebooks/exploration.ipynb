{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/nathalie/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/nathalie/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from Levenshtein import distance\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import words, brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary and word frequencies\n",
    "vocabulary = set(words.words())  # List of valid words\n",
    "word_frequencies = Counter(brown.words())  # Word frequencies from the Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Remove punctuation and lowercase text.\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word.lower() for word in brown.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Counter(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i NN\n",
      "['i', 'a', 'in', 'is', 'it']\n",
      "am VBP\n",
      "['am', 'a', 'as', 'at', 'an']\n",
      "beginning VBG\n",
      "['beginning', 'ginning', 'unbeginning', 'rebeginning', 'binning']\n",
      "to TO\n",
      "['to', 'no', 'so', 'two', 'do']\n",
      "wamder NN\n",
      "['wander', 'warder', 'wadder', 'wader', 'water']\n",
      "if IN\n",
      "['if', 'of', 'in', 'is', 'it']\n",
      "lonjer NN\n",
      "['longer', 'lower', 'wonder', 'lover', 'locker']\n",
      "sentinces NNS\n",
      "['sentence', 'sentinel', 'sentencer', 'sentience', 'Bentincks']\n",
      "wuld NN\n",
      "['would', 'wild', 'weld', 'wold', 'suld']\n",
      "work NN\n",
      "['work', 'word', 'works', 'wore', 'worn']\n",
      "bether NN\n",
      "['better', 'bother', 'ether', 'nether', 'yether']\n",
      "for IN\n",
      "['for', 'or', 'For', 'far', 'form']\n",
      "tis NN\n",
      "['is', 'his', 'this', 'tie', 'tip']\n",
      "prgramn NN\n",
      "['program', 'predamn', 'grain', 'organ', 'gram']\n",
      "i am beginning to wander if longer sentence would work better for is program\n"
     ]
    }
   ],
   "source": [
    "# Load vocabulary and word frequencies\n",
    "# vocabulary = set(words.words())  # List of valid words\n",
    "word_frequencies = Counter(brown.words())  # Word frequencies from the Brown corpus\n",
    "\n",
    "model = Counter(ngrams(tokens, 3))\n",
    "\n",
    "# input = 'cheking speling is complicatek'\n",
    "# input = 'my mathwr is angru'\n",
    "# input = 'the bes are vry fastt whn they are flyinh'\n",
    "# input = 'the girl has multipe dresses'\n",
    "# input = 'korrectud'\n",
    "input = 'i am beginning to wamder if lonjer sentinces wuld work bether for tis prgramn'\n",
    "input_preprocessed = preprocess_text(input)\n",
    "words = input_preprocessed.split()\n",
    "\n",
    "corrected_words = []\n",
    "prev_word = None\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    # if word in vocabulary:\n",
    "    #     corrected_words.append(word)\n",
    "    #     prev_word = word\n",
    "    #     continue\n",
    "\n",
    "    word_tag = pos_tag([word])[0][1]\n",
    "    print(word, word_tag)\n",
    " \n",
    "    # Generate suggestions\n",
    "    suggestions = [\n",
    "        (vocab_word, distance(word, vocab_word), word_frequencies[vocab_word])\n",
    "        for vocab_word in vocabulary\n",
    "    ]\n",
    "    suggestions = sorted(suggestions, key=lambda x: (x[1], -x[2]))\n",
    "    suggestions = [suggestion[0] for suggestion in suggestions[0:5]]\n",
    "    print(suggestions)\n",
    "\n",
    "    # Initialize variables\n",
    "    best_suggestion = suggestions[0]\n",
    "    best_combined_prob = 0\n",
    "\n",
    "    # Get the next word (if it exists)\n",
    "    next_word = words[i + 1] if i + 1 < len(words) else None\n",
    "\n",
    "    for suggestion in suggestions:\n",
    "        # Calculate bigram probability with the previous word\n",
    "        prob_prev = 0\n",
    "        if prev_word:\n",
    "            bigram_count = model.get((prev_word, suggestion), 0)\n",
    "            unigram_count = sum(1 for bigram in model if bigram[0] == prev_word)\n",
    "            prob_prev = bigram_count / unigram_count if unigram_count > 0 else 0\n",
    "\n",
    "        # Calculate bigram probability with the next word\n",
    "        prob_next = 0\n",
    "        if next_word:\n",
    "            bigram_count = model.get((suggestion, next_word), 0)\n",
    "            unigram_count = sum(1 for bigram in model if bigram[0] == suggestion)\n",
    "            prob_next = bigram_count / unigram_count if unigram_count > 0 else 0\n",
    "\n",
    "        # Combine probabilities (you can adjust weights as needed)\n",
    "        combined_prob = (prob_prev + prob_next) / 2\n",
    "\n",
    "        # Update best suggestion based on combined probability\n",
    "        if combined_prob > best_combined_prob:\n",
    "            best_combined_prob = combined_prob\n",
    "            best_suggestion = suggestion\n",
    "\n",
    "    corrected_words.append(best_suggestion)\n",
    "    prev_word = best_suggestion  # Update previous word\n",
    "\n",
    "corrected_text = ' '.join(corrected_words)\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spellchecker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
